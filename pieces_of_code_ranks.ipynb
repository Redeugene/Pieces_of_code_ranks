{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a548397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "import cufflinks as cf  \n",
    "import configparser as cp\n",
    "import psycopg2\n",
    "import pandas.io.sql as sqlio\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine\n",
    "import configparser\n",
    "import sqlalchemy\n",
    "from time import sleep\n",
    "from sklearn.impute import KNNImputer\n",
    "from featurewiz import featurewiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a connection with a SQL DB\n",
    "username = 'postgres'  # DB username\n",
    "password = '270122'  # DB password\n",
    "host = 'localhost'  # Public IP address for your instance\n",
    "port = '5432'\n",
    "database = 'postgres'  # Name of database\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(host=host,\n",
    "                        database=database,\n",
    "                        user=username,\n",
    "                        password=password)\n",
    "cursor = conn.cursor()\n",
    "db_url = 'postgresql+psycopg2://{}:{}@{}:{}/{}'.format(\n",
    "    username, password, host, port, database)\n",
    "engine = sqlalchemy.create_engine(db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8062b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select some data from DB\n",
    "price_old = pd.DataFrame()\n",
    "for ric in rics:\n",
    "    ric = \"\\'\" + ric + \"\\'\"\n",
    "    date = \"\\'\" + start_date + \"\\'\"\n",
    "    price_old = price_old.append(sqlio.read_sql_query('select * from pricesdaily where \"compId\" = '  + ric +\n",
    "                         ' and \"price\" is not Null and \"datePrice\" < ' + date + ' order by \"datePrice\" desc limit 1;', conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdbf201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete some data of DB\n",
    "for ric in rics_to_update:\n",
    "    try:\n",
    "        cursor.execute('DELETE FROM ' + name_table_with_old_prices + ''' WHERE \"compId\" = '%s' ''' % (ric))\n",
    "        conn.commit()\n",
    "    except:\n",
    "        cursor.execute(\"rollback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0151171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert some data in the a table of DB\n",
    "price_old.to_sql(name_table_with_old_prices, engine, index=False, if_exists= 'append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a94c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get slice of data by several conditions\n",
    "subset = price_new.Instrument.isin(rics_to_not_update)\n",
    "price_new_sliced = price_new[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b1c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datetime to date: datetime.datetime(2022, 3, 28, 0, 0) -> datetime.date(2022, 3, 28)\n",
    "price['Date'] = price['Date'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bdc4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String of datetime to date: '2022-3-28 0:00:00' -> datetime.date(2022, 3, 28)\n",
    "data['Date'] = data['Date'].apply(lambda x: datetime.strptime(x[:10], \"%Y-%m-%d\").date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find relative changes of only some features of dataframe (price and price_spx)\n",
    "sort_full = full_df.sort_values(['RIC', 'Date'])\n",
    "sort_full.index = range(len(sort_full))\n",
    "pct = sort_full[['price', 'price_spx']].pct_change()\n",
    "\n",
    "voc_uniq_rics = {}\n",
    "for ric in sort_full.RIC.unique().tolist():\n",
    "    voc_uniq_rics[ric] = []\n",
    "for row in range(len(sort_full)):\n",
    "    voc_uniq_rics[sort_full.RIC[row]] += [row]\n",
    "\n",
    "useless_rows = []\n",
    "for useless_row in voc_uniq_rics.values():\n",
    "    useless_rows.append(useless_row[0])\n",
    "useless_rows\n",
    "\n",
    "full_pct = pd.concat([sort_full.drop(columns = ['price', 'price_spx']), pct], axis = 1)\n",
    "full_pct.drop(index = useless_rows, inplace = True)\n",
    "full_pct.index = range(len(full_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072402ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with many nans\n",
    "voc_nan = {}\n",
    "for col in russia.columns.tolist():\n",
    "    voc_nan[col] = len(russia[russia[col].isna()])\n",
    "voc_nan_pd = pd.DataFrame(voc_nan, index = [0]).T\n",
    "useful_cols = voc_nan_pd[voc_nan_pd[0] < 0.7 * len(russia)].T.columns.tolist()\n",
    "russia = russia[useful_cols]\n",
    "\n",
    "# Drop rows with many nans\n",
    "russia = russia.dropna(thresh = len(russia.columns.tolist()) // 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44829447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step of filling nans grouped by only companies\n",
    "for i in russia.drop(columns = ['MCap_group', 'gicsSectorName', 'Country_of_Headquarters', 'RIC', 'Date'], errors = 'ignore').columns.tolist():\n",
    "    russia[i] = russia.groupby(['RIC'])[i].transform(lambda x: x.fillna(method = 'ffill'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step of filling nans grouped by capitalisations of companies\n",
    "for i in russia.drop(columns = ['MCap_group', 'gicsSectorName', 'Country_of_Headquarters', 'RIC', 'Date'], errors = 'ignore').columns.tolist():\n",
    "    russia[i] = russia.groupby(['MCap_group'])[i].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd1d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of filling empty values with using K-nearest neighbours\n",
    "imputer = KNNImputer(n_neighbors = 1)\n",
    "df_alfa = imputer.fit_transform(df_alfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efb94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into 5 equal parts\n",
    "russia['MCap_group'] = pd.cut(russia['Company_Market_Cap'], 5, labels = range(5))\n",
    "\n",
    "# Divide into 5 parts with equal number of elements\n",
    "russia['MCap_group'] = pd.qcut(russia['Company_Market_Cap'], 5, labels = range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdcb5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectring important features:\n",
    "\n",
    "# 1.: MRMR and Recursive gradient boosting\n",
    "selected_cols, imp_df = featurewiz(dataframe, 'alfa', corr_limit=0.55, verbose =0, sep=\",\", header=0,test_data=\"\", feature_engg=\"\", category_encoders=\"\")\n",
    "\n",
    "# 2.: By feature importances_ (Recursive gradient boosting with manual settings):\n",
    "model_xgb = XGBRegressor(max_depth = 6, n_jobs = -1, use_label_encoder = False, learning_rate = 0.1, gamma = 1, n_estimates = 300)\n",
    "model_xgb.fit(x_train, y_train)\n",
    "top_pd = pd.DataFrame(model_xgb.feature_importances_, index = x.columns.tolist()).sort_values(0, ascending = False)\n",
    "    \n",
    "cum_sum = 0\n",
    "for col in top_pd.index.tolist():\n",
    "    if top_pd.loc[col, 0] < 0.01 or cum_sum >= 0.6:\n",
    "        break\n",
    "    last_col = col\n",
    "    cum_sum += top_pd.loc[col, 0]\n",
    "    \n",
    "selected_cols = top_pd.loc[:last_col, :].index.tolist()\n",
    "consist = str(top_pd.loc[:last_col, :].to_dict()[0])\n",
    "\n",
    "# 3.: Weight of evidence:\n",
    "for col in small_zeroes_df.drop(columns = ['gicsSectorName',\n",
    "                                                  'Country_of_Headquarters', 'MCap_group', 'alfa']).columns.tolist():\n",
    "    small_zeroes_df[col] = pd.qcut(small_zeroes_df[col], 5, labels = False, duplicates = 'drop')\n",
    "\n",
    "    def calculate_woe_iv(dataset, feature, target):\n",
    "    lst = []\n",
    "    for i in range(dataset[feature].nunique()):\n",
    "        val = list(dataset[feature].unique())[i]\n",
    "        lst.append({\n",
    "            'Value': val,\n",
    "            'All': dataset[dataset[feature] == val].count()[feature],\n",
    "            'Good': dataset[(dataset[feature] == val) & (dataset[target] == 1)].count()[feature],\n",
    "            'Bad': dataset[(dataset[feature] == val) & (dataset[target] == 0)].count()[feature]\n",
    "        })\n",
    "        \n",
    "    dset = pd.DataFrame(lst)\n",
    "    dset['Distr_Good'] = dset['Good'] / dset['Good'].sum()\n",
    "    dset['Distr_Bad'] = dset['Bad'] / dset['Bad'].sum()\n",
    "    dset['WoE'] = np.log(dset['Distr_Good'] / dset['Distr_Bad'])\n",
    "    dset = dset.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n",
    "    dset['IV'] = (dset['Distr_Good'] - dset['Distr_Bad']) * dset['WoE']\n",
    "    iv = dset['IV'].sum()\n",
    "    \n",
    "    dset = dset.sort_values(by='WoE') \n",
    "    return dset, iv\n",
    "\n",
    "for col in small_zeroes_df.columns:\n",
    "    if col == 'alfa': continue\n",
    "    else:\n",
    "        print('WoE and IV for column: {}'.format(col))\n",
    "        df, iv = calculate_woe_iv(small_zeroes_df, col, 'alfa')\n",
    "        print(df)\n",
    "        print('IV score: {:.2f}'.format(iv))\n",
    "        print('\\n')\n",
    "        \n",
    "# 4.: By p-value:\n",
    "x_incl_const = sm.add_constant(x_train)\n",
    "model = sm.OLS(y_train, x_incl_const)\n",
    "results = model.fit()\n",
    "p_df = pd.DataFrame({'coef':results.params, 'p-value':round(results.pvalues, 3), 'param':['const']+x.columns.tolist()})\n",
    "p_df[p_df['p-value'] < 0.05]\n",
    "print(results.bic)\n",
    "results.rsquared\n",
    "\n",
    "      # after deleting some features need to find out BIC\n",
    "x = small_zeroes_df.drop(columns = ['gicsSectorName', 'Country_of_Headquarters', 'alfa'] + unimportant_cols)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(x_train)\n",
    "x_train = scaler_x.transform(x_train)\n",
    "x_test = scaler_x.transform(x_test)\n",
    "x_incl_const = sm.add_constant(x_train)\n",
    "model = sm.OLS(y_train, x_incl_const)\n",
    "results = model.fit()\n",
    "#pd.DataFrame({'coef':results.params, 'p-value':round(results.pvalues, 3), 'param':['const']+x.columns.tolist()})\n",
    "print(results.bic)\n",
    "results.rsquared\n",
    "\n",
    "# 5.: Dispersion of proving params\n",
    "x = small_zeroes_df.drop(columns = ['alfa'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "x_incl_const = sm.add_constant(x_train)\n",
    "vif = pd.DataFrame([variance_inflation_factor(exog = x_incl_const.values, exog_idx = i)\n",
    "               for i in range(x_incl_const.shape[1])],\n",
    "              index=['const']+x.columns.tolist())\n",
    "without_mult_cols = vif[vif[0] < 5].drop(index = 'const', errors = 'ignore').index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81526ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an offset of column grouped by another one\n",
    "full_pct['alfa'] = full_pct.groupby('RIC')['alfa'].shift(periods = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d35d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the used RAM by dataframe in GiB\n",
    "data_edited_cols.memory_usage(deep = True).sum()/1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique values from list\n",
    "rics = list(set(rics_df.comId.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of target and factor features with ordering and plotting\n",
    "corr_map = data.corr()\n",
    "print(corr_map['Price_Close'].sort_values(ascending = False))\n",
    "plt.figure(figsize = (14,14))\n",
    "sns.heatmap(corr_map, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace84296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standatization of values\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(x_train)\n",
    "x_train = scaler_x.transform(x_train)\n",
    "x_test = scaler_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3695cdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading a model to preddict values\n",
    "import pickle\n",
    "file_name = \"xgb_model_version_1.pkl\"\n",
    "# save\n",
    "pickle.dump(model_xgb, open(file_name, \"wb\"))\n",
    "# load\n",
    "model_xgb = pickle.load(open(file_name, \"rb\"))\n",
    "y_pred_xgb = model_xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057afd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix and find the precision score\n",
    "cm = confusion_matrix(y_test, y_pred_xgb, labels=model_xgb.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                           display_labels=model_xgb.classes_)\n",
    "disp.plot()\n",
    "print('precision_xgb = ', cm[1][1]/(cm[1][1] + cm[0][1]) * 100, ' %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11830bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df by subsets of sectors of countries\n",
    "small_countries = []\n",
    "small_sectors = []\n",
    "testing = small_zeroes_df.copy()\n",
    "train = pd.DataFrame(columns = testing.columns)\n",
    "test = pd.DataFrame(columns = testing.columns)\n",
    "for country in testing.Country_of_Headquarters.unique().tolist():\n",
    "    for sector in testing.gicsSectorName.unique().tolist():\n",
    "        segment_data = testing[(testing.Country_of_Headquarters == country) & \n",
    "                                      (testing.gicsSectorName == sector)]\n",
    "        \n",
    "        segment_data.index = range(len(segment_data))\n",
    "        if len(segment_data) < 30:\n",
    "            train = train.append(segment_data)\n",
    "            small_countries.append(country)\n",
    "            small_sectors.append(sector)\n",
    "        thresh = len(segment_data) * 9 // 10\n",
    "        train = train.append(segment_data.loc[:thresh - 1, :])\n",
    "        test = test.append(segment_data.loc[thresh:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021f739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert a variable into a string\n",
    "name_imp = f'usa_sector_{sector}_imp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort of list\n",
    "sectors_usa = sorted(usa_test['gicsSectorName'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb718f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List generator\n",
    "data_fund['alfa'] = [1 if x > 0 else 0 for x in data_fund.alfa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96265207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From object type to category\n",
    "data_fund['Country_of_Headquarters_cat'] = data_fund['Country_of_Headquarters'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8764c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice by types\n",
    "data_fund.select_dtypes(include=[object])\n",
    "data_fund.select_dtypes(exclude=[object])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b8de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of values of categorical feature\n",
    "usa_sector.alfa.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552d41a",
   "metadata": {},
   "outputs": [],
   "source": ["# Round to any time\n",
             "def roundTime(dt=None, roundTo=60*30):
   """Round a datetime object to any time lapse in seconds
   dt : datetime.datetime object, default now.
   roundTo : Closest number of seconds to round to, default 1 minute.
   Author: Thierry Husson 2012 - Use it as you want but don't blame me.
   """
   if dt == None : dt = datetime.datetime.now()
   seconds = (dt.replace(tzinfo=None) - dt.min).seconds
   rounding = (seconds+roundTo/2) // roundTo * roundTo
   return dt + timedelta(0,rounding-seconds,-dt.microsecond)"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72b8de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
